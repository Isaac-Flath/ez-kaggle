{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ez_kaggle.dataset\n",
    "\n",
    "> API details for using datasets to store competition related things (model weights, pip libraries, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import json, os, subprocess, shutil\n",
    "from pathlib import Path\n",
    "from ez_kaggle.setup import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.foundation import L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ds_exists(dataset_slug, # Dataset slug (ie \"zillow/zecon\")\n",
    "                   path='.' # path to fastkaggle.json file or None\n",
    "             ):\n",
    "    '''Check if a dataset exists'''\n",
    "    md_path = Path(Path(path)/'dataset-metadata.json')\n",
    "    assert not md_path.exists(),'dataset-metadata.json already exists. Use a path that is not a kaggle dataset'\n",
    "    try: \n",
    "        api=import_kaggle()\n",
    "        api.dataset_metadata(dataset_slug,path)\n",
    "        md_path.unlink()\n",
    "        return True\n",
    "    except Exception as ex:\n",
    "        if '404' in str(ex): return False\n",
    "        else: raise ex  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ds_exists('isaacflath/library-fastkaggle')\n",
    "assert not ds_exists('not/real/dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_dataset(dataset_path, # Local path to create dataset in\n",
    "               title, # Name of the dataset\n",
    "               force=False, # Should it overwrite or error if exists?\n",
    "               upload=True, # Should it upload and create on kaggle\n",
    "               cfg_path='.', # path to fastkaggle.json file or None\n",
    "               **kwargs # Config dict to overwrite or replace fastkaggle.json\n",
    "              ):\n",
    "    '''Creates minimal dataset metadata needed to push new dataset to kaggle'''\n",
    "    cfg = get_config_values(cfg_path,**kwargs)\n",
    "    dataset_path = Path(dataset_path)\n",
    "    dataset_path.mkdir(exist_ok=force,parents=True)\n",
    "    api = import_kaggle()\n",
    "    api.dataset_initialize(dataset_path)\n",
    "    md = json.load(open(dataset_path/'dataset-metadata.json'))\n",
    "    md['title'] = title\n",
    "    md['id'] = md['id'].replace('INSERT_SLUG_HERE',title)\n",
    "    json.dump(md,open(dataset_path/'dataset-metadata.json','w'))\n",
    "    if upload: (dataset_path/'empty.txt').touch()\n",
    "    api.dataset_create_new(str(dataset_path),public=True,dir_mode='zip',quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data package template written to: testds/dataset-metadata.json\n"
     ]
    }
   ],
   "source": [
    "mk_dataset('./testds','mytestds',force=True,upload=False)\n",
    "path = Path('./testds/dataset-metadata.json')\n",
    "md = json.load(open(path))\n",
    "assert md['title'] == 'mytestds'\n",
    "assert md['id'].endswith('/mytestds')\n",
    "path.unlink()\n",
    "path.parent.rmdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_dataset(dataset_slug, # Dataset slug (ie \"zillow/zecon\")\n",
    "                dataset_path, # Local path to download dataset to\n",
    "                unzip=True, # Should it unzip after downloading?\n",
    "                force=False # Should it overwrite or error if dataset_path exists?\n",
    "               ):\n",
    "    '''Downloads an existing dataset and metadata from kaggle'''\n",
    "    if not force: assert not Path(dataset_path).exists()\n",
    "    api = import_kaggle()\n",
    "    api.dataset_metadata(dataset_slug,str(dataset_path))\n",
    "    api.dataset_download_files(dataset_slug,str(dataset_path))\n",
    "    if unzip:\n",
    "        zipped_file = Path(dataset_path)/f\"{dataset_slug.split('/')[-1]}.zip\"\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile(zipped_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(Path(dataset_path))\n",
    "        zipped_file.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path('./data-science-job-salaries')\n",
    "get_dataset('ruchi798/data-science-job-salaries',dataset_path, force=True)\n",
    "\n",
    "files = os.listdir(dataset_path)\n",
    "\n",
    "assert L(files).sorted() == ['dataset-metadata.json', 'ds_salaries.csv']\n",
    "\n",
    "for f in Path(dataset_path).ls(): f.unlink()\n",
    "Path(dataset_path).rmdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def push_dataset(dataset_path, # Local path where dataset is stored \n",
    "                 version_comment, # Comment associated with this dataset update\n",
    "                quiet=True\n",
    "                ):\n",
    "    '''Push dataset update to kaggle.  Dataset path must contain dataset metadata file'''\n",
    "    api = import_kaggle()\n",
    "    api.dataset_create_version(str(dataset_path),version_comment,dir_mode='zip',quiet=quiet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pip Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_pip_library(pip_library, # name of library for pip to install\n",
    "                    cfg_path='.', # path to fastkaggle.json file or None\n",
    "                     **kwargs # Config dict to overwrite or replace fastkaggle.json\n",
    "                   ):    \n",
    "    '''Download the whl files for pip_library and store in dataset_path'''\n",
    "    cfg = get_config_values(cfg_path,**kwargs)\n",
    "    \n",
    "    pip_cmd=cfg['pip_cmd']\n",
    "    dataset_path = Path(cfg_path)/cfg['data_path']/pip_library\n",
    "\n",
    "    bashCommand = f\"{pip_cmd} download {pip_library} -d {dataset_path}\"\n",
    "    process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "    output, error = process.communicate()\n",
    "    return process,output,error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib = 'fastcore'\n",
    "get_pip_library(lib)\n",
    "assert Path(lib).exists()\n",
    "Path(lib).ls().map(lambda x: x.unlink())\n",
    "Path(lib).rmdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_pip_libraries(directory_name,\n",
    "                     cfg_path='.', # path to fastkaggle.json file or None\n",
    "                     **kwargs # Config dict to overwrite or replace fastkaggle.json\n",
    "                   ):    \n",
    "    cfg = get_config_values(cfg_path,**kwargs)\n",
    "    \n",
    "    pip_cmd=cfg['pip_cmd']\n",
    "    dataset_path = Path(cfg_path)/cfg['data_path']/directory_name\n",
    "    libraries = ' '.join(cfg['required_libraries'])\n",
    "\n",
    "    bashCommand = f\"{pip_cmd} download {libraries} -d {dataset_path}\"\n",
    "    process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "    output, error = process.communicate()\n",
    "    return process,output,error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_name = 'my-test-libs'\n",
    "get_pip_libraries('my-test-libs')\n",
    "assert Path(directory_name).exists()\n",
    "Path(directory_name).ls().map(lambda x: x.unlink())\n",
    "Path(directory_name).rmdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_local_ds_ver(lib_path, # Local path dataset is stored in\n",
    "                     lib # Name of library (ie \"fastcore\")\n",
    "                    ):\n",
    "    '''checks a local copy of kaggle dataset for library version number'''\n",
    "    wheel_lib_name = lib.replace('-','_')\n",
    "    local_path = (lib_path/f\"library-{lib}\")\n",
    "    lib_whl = local_path.ls().filter(lambda x: wheel_lib_name in x.name.lower())\n",
    "    if 1==len(lib_whl):\n",
    "        return re.search(f\"(?<={wheel_lib_name}-)[\\d+.]+\\d\",lib_whl[0].name.lower())[0]\n",
    "    elif 0<len(local_path.ls().filter(lambda x: 'dist' in x.name)):\n",
    "        lib_whl = (local_path/'dist').ls().filter(lambda x: wheel_lib_name in x.name.lower())\n",
    "        if 1==len(lib_whl):\n",
    "            return re.search(f\"(?<={wheel_lib_name}-)[\\d+.]+\\d\",lib_whl[0].name.lower())[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_dependency_dataset(version_notes = \"New Update\",\n",
    "                              cfg_path='.', # path to fastkaggle.json file or None\n",
    "                              **kwargs # Config dict to overwrite or replace fastkaggle.json\n",
    "                           ):        \n",
    "    retain = [\"dataset-metadata.json\"]\n",
    "    cfg = get_config_values(cfg_path,**kwargs)\n",
    "    \n",
    "    pip_cmd=cfg['pip_cmd']\n",
    "    local_path = Path(cfg_path)/cfg['data_path']/cfg['libraries_dataset_name']\n",
    "    ds_slug = f\"{cfg['datasets_username']}/{cfg['libraries_dataset_name']}\"\n",
    "    \n",
    "    print(f\"-----Downloading or Creating Dataset if needed\")\n",
    "    if local_path.exists(): pass\n",
    "    elif ds_exists(ds_slug): get_dataset(ds_slug,str(local_path))\n",
    "    else:                    mk_dataset(local_path,cfg['libraries_dataset_name'])\n",
    "    \n",
    "    print(f\"-----Checking dataset files against pip\")\n",
    "    orig_ds = Path(local_path).ls().sorted()\n",
    "    for item in local_path.ls():\n",
    "        if item.name in retain: pass\n",
    "        elif item.is_dir(): shutil.rmtree(item)\n",
    "        else: item.unlink()      \n",
    "    get_pip_libraries(cfg['libraries_dataset_name'],cfg_path) \n",
    "    new_ds = Path(local_path).ls().sorted()\n",
    "    \n",
    "    if orig_ds != new_ds: \n",
    "        print(f\"-----Updating {cfg['libraries_dataset_name']} in Kaggle\")\n",
    "        push_dataset(local_path,version_notes)\n",
    "    else: print(f\"-----Kaggle dataset already up to date\")\n",
    "    print(f\"{ds_slug} update complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Downloading or Creating Dataset if needed\n",
      "-----Checking dataset files against pip\n",
      "-----Updating libraries-titanic in Kaggle\n",
      "isaacflath/libraries-titanic update complete\n"
     ]
    }
   ],
   "source": [
    "create_dependency_dataset()\n",
    "path = Path('libraries-titanic')\n",
    "assert path.exists()\n",
    "assert ds_exists('isaacflath/libraries-titanic')\n",
    "ds_exists('isaacflath/libraries-titanic')\n",
    "Path(path).ls().map(lambda x: x.unlink())\n",
    "Path(path).rmdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def push_fastai_learner(learner, # Fastai Learner\n",
    "                        model_fname, # ie `model1.pkl`\n",
    "                        version_comment, # dataset versioning\n",
    "                        cfg_path='.', # path to fastkaggle.json file or None\n",
    "                        **kwargs # Config dict to overwrite or replace fastkaggle.json\n",
    "                           ):        \n",
    "    '''Exports a learner and updates kaggle dataset'''\n",
    "    cfg = get_config_values(cfg_path,**kwargs)\n",
    "    \n",
    "    local_path = Path(cfg_path)/cfg['data_path']/cfg['model_dataset_name']\n",
    "    ds_slug = f\"{cfg['datasets_username']}/{cfg['model_dataset_name']}\"\n",
    "    \n",
    "    print(f\"-----Downloading or Creating Dataset if needed\")\n",
    "    if local_path.exists(): pass\n",
    "    elif ds_exists(ds_slug): get_dataset(ds_slug,str(local_path))\n",
    "    else:                    mk_dataset(local_path,cfg['model_dataset_name'])\n",
    "    \n",
    "    print(local_path)\n",
    "    orig_path = learner.path\n",
    "    learner.path = local_path\n",
    "    learner.export(model_fname)\n",
    "    learner.path = orig_path\n",
    "    push_dataset(local_path,version_comment)\n",
    "    print(f\"{ds_slug} update complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Downloading or Creating Dataset if needed\n",
      "models-titanic\n",
      "isaacflath/models-titanic update complete\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "import pandas as pd\n",
    "\n",
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "df = pd.read_csv(path/'labels.csv')\n",
    "dls = ImageDataLoaders.from_df(df,path)\n",
    "learn = vision_learner(dls, models.resnet18, loss_func=CrossEntropyLossFlat(), ps=0.25)\n",
    "\n",
    "push_fastai_learner(learn,'model1.pkl','testing fastkaggle')\n",
    "\n",
    "path = Path('models-titanic')\n",
    "assert path.exists()\n",
    "assert ds_exists('isaacflath/models-titanic')\n",
    "Path(path).ls().map(lambda x: x.unlink())\n",
    "Path(path).rmdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
